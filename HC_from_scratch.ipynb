{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddcfe676",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import heapq\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import sys\n",
    "import random\n",
    "import heapq\n",
    "import ipynb.fs  \n",
    "sys.path.append(\"../\")\n",
    "from .defs.get_abstract_2 import count_shared_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f01220b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_2430/3193620936.py:10: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  papers = pd.read_csv(\"../paper.csv\")\n"
     ]
    }
   ],
   "source": [
    "with open('../MADStat-dataset-final-version/data.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "'''load list of authors'''\n",
    "with open('../author_name.txt') as f:\n",
    "    authors = f.readlines()\n",
    "authors = [author.strip() for author in authors]\n",
    "\n",
    "'''load papers info'''\n",
    "papers = pd.read_csv(\"../paper.csv\")\n",
    "\n",
    "\"\"\"load list of authors having at least 30 papers\"\"\"\n",
    "with open(\"../../authors\",\"rb\") as fp:\n",
    "    author_l = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c44ae425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def HC(pvals, gamma=0.2, thresh=0.4):\n",
    "    pvals = np.sort(pvals[pvals <= thresh])\n",
    "    N = len(pvals)\n",
    "    hc = -1000\n",
    "    i_star = 0\n",
    "    for i in range(1,int(gamma*N)+1):\n",
    "        if pvals[i-1] >= 1/N:\n",
    "            num = np.sqrt(N)*((i/N) - pvals[i-1])\n",
    "            den = np.sqrt((i/N)*(1-i/N))\n",
    "            cur = num/den\n",
    "            if cur > hc:\n",
    "                hc = cur\n",
    "                i_star = i\n",
    "    return hc, i_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2868c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(data) :\n",
    "        #data.text = data.text.apply(remove_hexa_symbols)\n",
    "        #data.text = data.text.apply(remove_digits)\n",
    "        data = data.filter(['author', 'title', 'text']).rename(columns = {'title' : 'doc_id'})\n",
    "        data[\"len\"] = data.text.apply(lambda x: len(x))\n",
    "        data.text = data.text.apply(lambda x: re.sub(\"All rights\",\"\",x))\n",
    "        data.text = data.text.apply(lambda x: re.sub(\"reserved\",\"\",x))\n",
    "#         data.text = data.text.apply(lambda x: re.sub(\"[0-9]\",\"\",x))\n",
    "        data.text = data.text.apply(lambda x: re.sub(\"[^A-Za-z ]\",\"\",x))\n",
    "        data.text = data.text.apply(lambda x: re.sub(\"copyright\",\"\",x))\n",
    "        data.text = data.text.apply(lambda x: x.lower())\n",
    "        data = data.loc[data.len > 10].reset_index()\n",
    "        data.drop(columns=[\"len\"],inplace=True)\n",
    "        return data\n",
    "    \n",
    "def topKFrequent(nums, k):\n",
    "    dic=Counter(nums)\n",
    "    heapmax=[[-freq,num] for num,freq in dic.items()]\n",
    "    heapq.heapify(heapmax)\n",
    "    list1=[]\n",
    "    for i in range(k):\n",
    "        poping=heapq.heappop(heapmax)\n",
    "        list1.append(poping[1])\n",
    "    return list1\n",
    "\n",
    "\n",
    "def get_vocab(text, max_length=200):\n",
    "#     clf = CountVectorizer(lowercase=True)\n",
    "#     clf.fit([text])\n",
    "#     vocab = list(clf.vocabulary_.keys())\n",
    "#     print(\"vocab before = \",vocab)\n",
    "    vocab = text.split()\n",
    "    k = min(max_length, len(set(vocab)))\n",
    "#     return heapq.nlargest(k, vocab, key=vocab.get)\n",
    "#     print(vocab)\n",
    "    return topKFrequent(vocab,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d0237d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:  - text is a list of strings corresponding to documents\n",
    "        - vocab is the vocabulary used for the problem\n",
    "\"\"\"\n",
    "def doc_to_dtm(text, vocab):\n",
    "    #tk = TweetTokenizer()\n",
    "    vectorizer = CountVectorizer(tokenizer=lambda txt: txt.split(),vocabulary=vocab) #tokenizer=tk.tokenize,\n",
    "#     X = vectorizer.fit_transform(text)\n",
    "    X = vectorizer.transform(text)\n",
    "    return X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "924b2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_poisson(corpus):\n",
    "    return np.mean(corpus,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5cbf008",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Return pvals using standard normal cdf\"\"\"\n",
    "def get_pvals(author1,author2,show_hist=False,thresh=0.4):\n",
    "    \n",
    "    def replace_labels(x):\n",
    "        x[x==author1] = 1\n",
    "        x[x==author2] = 0\n",
    "        return x\n",
    "\n",
    "    author_1 = pd.read_csv(f'../Data/{author1}.csv').filter(['author', 'title', 'text'])\n",
    "    author_2 = pd.read_csv(f'../Data/{author2}.csv').filter(['author', 'title', 'text'])\n",
    "    n, m = author_1.shape[0], author_2.shape[0]\n",
    "    if author1 != author2 and count_shared_papers(author1,author2,authors,data)==0 and min(n/m, m/n) >= 1/2:   \n",
    "        data_ = pd.concat([clean_text(author_1),\n",
    "                                          clean_text(author_2)], ignore_index=True)\n",
    "\n",
    "        data_train = data_.sample(frac=0.7)\n",
    "        data_test = data_.drop(data_train.index)\n",
    "        vocab = get_vocab(''.join([doc + \" \" for doc in list(data_train[\"text\"])]), max_length=400)\n",
    "\n",
    "\n",
    "        text1 = data_train[data_train[\"author\"]==author1]\n",
    "        text2 = data_train[data_train[\"author\"]==author2]\n",
    "\n",
    "        corpus1 = doc_to_dtm(list(text1.text),vocab=vocab)\n",
    "        corpus2 = doc_to_dtm(list(text2.text),vocab=vocab)\n",
    "        corpus_test = doc_to_dtm(list(data_test.text),vocab=vocab)\n",
    "        \n",
    "        lam_1 = estimate_poisson(corpus1)\n",
    "        lam_2 = estimate_poisson(corpus2)\n",
    "        \n",
    "        sx = np.std(corpus1,axis=0)\n",
    "        sy = np.std(corpus2,axis=0)\n",
    "        z = (lam_1 - lam_2)/np.sqrt((sx**2/corpus1.shape[0]) + (sy**2/corpus2.shape[0]))\n",
    "        z_n = (z - np.mean(z))/np.std(z)\n",
    "        if show_hist:\n",
    "            plt.hist(z_n)\n",
    "            plt.title(f\"Normalized z-counts for {author1} and {author2}\")\n",
    "            plt.show()\n",
    "        pvals = 1 - norm.cdf(z_n)\n",
    "        hc, i_star = HC(pvals,thresh=thresh)\n",
    "        \n",
    "        # Prediction on test set\n",
    "        c1_hc = corpus1[:,pvals <= np.sort(pvals)[i_star]]\n",
    "        c2_hc = corpus2[:,pvals <= np.sort(pvals)[i_star]]\n",
    "        ct_hc = corpus_test[:,pvals <= np.sort(pvals)[i_star]]\n",
    "        \n",
    "        Z = evaluate(ct_hc,c1_hc,c2_hc)\n",
    "        y_preds = predict(Z)\n",
    "        y_true = replace_labels(np.array(data_test.author))   # 1 = author1, 0 = author2\n",
    "        print(f\"Accuracy on test set = {accuracy(y_preds,y_true)}\")\n",
    "        return y_preds, y_true\n",
    "    else:\n",
    "        return \"One author has more than twice the number of papers as the other one !!!\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d4fe71d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 1, 5])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,4])\n",
    "b = np.array([4,1,9])\n",
    "abs(a - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c175136a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author1=\"John Kent\"\n",
    "author2 = \"Iain Johnstone\"\n",
    "count=0\n",
    "for author in author_l:\n",
    "    if (author != author1) & (author!=author2):\n",
    "        c1 = count_shared_papers(author1,author,authors,data)\n",
    "        c2 = count_shared_papers(author2,author,authors,data)\n",
    "        if min(c1,c2) > 0:\n",
    "            count+=min(c1,c2)\n",
    "            \n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef93a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(new_data,c1,c2):\n",
    "    return new_data - ((c1.sum(axis=0) + c2.sum(axis=0))/(c1.shape[0]+c2.shape[0]))\n",
    "\n",
    "def predict(z):\n",
    "    return np.where(z.sum(axis=1) > 0, 1, 0)\n",
    "\n",
    "def accuracy(y_preds,y_true):\n",
    "    return np.mean(y_preds==y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fa5feb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set = 0.72\n",
      "(array([0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 1]), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0], dtype=object))\n"
     ]
    }
   ],
   "source": [
    "print(get_pvals(\"John Kent\",\"Iain Johnstone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "2d423dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/3691646764.py:7: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author 1</th>\n",
       "      <th>Author 2</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Han-ying Liang</td>\n",
       "      <td>Ngai Hang Chan</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Louise Ryan</td>\n",
       "      <td>Kerrie Mengersen</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jerzy K. Baksalary</td>\n",
       "      <td>Zehua Chen</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Enrique Schisterman</td>\n",
       "      <td>Paddy Farrington</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brian Caffo</td>\n",
       "      <td>Lyle Broemeling</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Myles Hollander</td>\n",
       "      <td>Victor De Gruttola</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Paul Gustafson</td>\n",
       "      <td>Noel Cressie</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Atanu Biswas</td>\n",
       "      <td>Noël Veraverbeke</td>\n",
       "      <td>0.965517</td>\n",
       "      <td>0.956522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Robert Kohn</td>\n",
       "      <td>Paul Janssen</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>David Schoenfeld</td>\n",
       "      <td>Stuart J. Pocock</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Author 1            Author 2  Accuracy        F1\n",
       "0       Han-ying Liang      Ngai Hang Chan  0.863636  0.842105\n",
       "1          Louise Ryan    Kerrie Mengersen  0.666667  0.700000\n",
       "2   Jerzy K. Baksalary          Zehua Chen  0.894737  0.888889\n",
       "3  Enrique Schisterman    Paddy Farrington  0.857143  0.666667\n",
       "4          Brian Caffo     Lyle Broemeling  0.700000  0.700000\n",
       "5      Myles Hollander  Victor De Gruttola  0.681818  0.631579\n",
       "6       Paul Gustafson        Noel Cressie  0.894737  0.857143\n",
       "7         Atanu Biswas    Noël Veraverbeke  0.965517  0.956522\n",
       "8          Robert Kohn        Paul Janssen  0.714286  0.692308\n",
       "9     David Schoenfeld    Stuart J. Pocock  0.900000  0.875000"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = pd.DataFrame()\n",
    "for i,pairs in enumerate(hard_pairs):\n",
    "    author1, author2 = pairs\n",
    "    y_true, y_preds = get_pvals(author1,author2)\n",
    "    acc = accuracy(y_true,y_preds)\n",
    "    f1 = f1_score(list(y_preds), list(y_true))\n",
    "    df_ = df_.append(pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2, \"Accuracy\":acc,\"F1\":f1},index=[i]))\n",
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd1578dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "author1 = \"Boxin Tang\"\n",
    "author_1 = pd.read_csv(f'../Data/{author1}.csv').filter(['author', 'title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f1f18d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c71f4c1bb64757b635be4a5f8f6ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce485fe162047da97b44896ad3f811f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5cc47f9d444f0db664e05d287bcc1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108e52d990e34a7c959a9b3687c27ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ec6d20a3f148ae8c3d23fd94671908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920e047479d54795bc238f8c8a2a4ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8c905957bef4ea3a9278e4f18406099",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b24ef42de1234e32a86e272df1f8c5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a65515221464679860b6888255932ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a12b70be05e443f2a5ff09b407a7d4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dbeba1afc89449c8474472126c77974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3997d0efe0447f69d00813ecd7d441b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7a7b55e09e496a84a62c0cdce943af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d71931471d404ec5a833d796f5536b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8bda293b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Two-level fractional factorial designs are considered under a baseline   parameterization. The criterion of minimum aberration is formulated in   this context and optimal designs under this criterion are investigated.   The underlying theory and the concept of isomorphism turn out to be   significantly different from their counterparts under orthogonal   parameterization, and this is reflected in the optimal designs obtained.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Orthogonal arrays with clear two-factor interactions provide a class of   designs that are robust to nonnegligible effects. If certain prior   knowledge is available, then robust designs allow additional factors to   be studied. This is done through partially clear two-factor   interactions. We study the existence and construction of such robust   designs and present an upper bound on the maximum number of clear   two-factor interactions.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: We introduce a method for constructing a rich class of designs that are   suitable for use in computer experiments. The designs include Latin   hypercube designs and two-level fractional factorial designs as special   cases and fill the vast vacuum between these two familiar classes of   designs. The basic construction method is simple, building a series of   larger designs based on a given small design. If the base design is   orthogonal, the resulting designs are orthogonal; likewise, if the base   design is nearly orthogonal, the resulting designs are nearly   orthogonal. We present two generalizations of our basic construction   method. The first generalization improves the projection properties of   the basic method; the second generalization gives rise to designs that   have smaller correlations. Sample constructions are presented and   properties of these designs are discussed.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: We propose a method for constructing orthogonal or nearly orthogonal   Latin hypercubes. The method yields a large Latin hypercube by coupling   an orthogonal array of index unity with a small Latin hypercube. It is   shown that the large Latin hypercube inherits the exact or near   orthogonality of the small Latin hypercube. Thus, effort for searching   for large Latin hypercubes, that are exactly or nearly orthogonal, can   be focussed on finding small Latin hypercubes with the same property. We   obtain a useful collection of orthogonal or nearly orthogonal Latin   hypercubes, which have a large factor-to-run ratio and the results are   often much more economical than existing methods.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Regular fractional factorial designs with clear two-factor interactions   provide a useful class of designs that are robust to nonnegligible   two-factor interactions. In this paper, the concept of clear two-factor   interactions is generalised to orthogonal arrays. The new concept leads   to a much wider class of designs robust to nonnegligible two-factor   interactions. We study the existence and construction of such designs.   The designs we construct have a structure that render themselves   particularly attractive in the robust parameter design setting. We also   discuss an interesting connection between designs with clear two-factor   interactions and mixed orthogonal arrays.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: The number of columns in of a two-level supersaturated design is at   least as large as its run size n, that is n less than or equal to in. In   this case, orthogonality among the columns of a design has to be   sacrificed. One measure of non-orthogonality among the columns of a   supersaturated design is given by max(i <j) /s(ij)/n/, where s(ij) =   d(i)(T)d(j) is the inner product of columns d(i) and d(j). An important   theoretical problem in this formulation is to find, for a given value 0   < r < 1, where r is the degree of nonorthogonality the experimenter is   willing to sacrifice, the maximum number M of columns such that max(i   <j) /s(ij)/n/ less than or equal to r. In this paper, we provide some   upper bounds on M. One such bound is obtained by connecting this problem   with that of the size of an error-correcting code. Another bound can be   derived from lower bounds on the E(s(2))-criterion which is defined as   Sigma (i <j)s(ij)(2)/\\(1)/(2)m(m - 1)\\. Some designs attaining the   upper bounds are also examined.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Deng \\& Tang (1999) introduced the generalised resolution and minimum   G-aberration criteria for assessing nonregular fractional factorials. In   Tang \\& Deng (1999), a relaxed variant of minimum G-aberration, called   minimum G(2)-aberration, is proposed and studied. These criteria are   defined using a set of J values, called J-characteristics. In this   paper, we show that a factorial design is uniquely determined by its   J-characteristics just as a regular factorial design is uniquely   determined by its defining relation. The theorem is given through an   explicit formula that relates the set of design points to that of   J-characteristics. Through this formula, projection justification of   minimum G(2)-aberration is established.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: We develop a method for construction of arrays which are nearly   orthogonal, in the sense that each column is orthogonal to a large   proportion of the other columns, and which are convertible to fully   orthogonal arrays via a mapping of the symbols in each column to a   possibly smaller set of symbols. These arrays can be useful in computer   experiments as designs which accommodate a large number of factors and   enjoy attractive space-filling properties. Our construction allows both   the mappable nearly orthogonal array and the consequent fully orthogonal   array to be either symmetric or asymmetric. Resolvable orthogonal arrays   play a key role in the construction.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This paper introduces, constructs and studies a new class of arrays,   called strong orthogonal arrays, as suitable designs for computer   experiments. A strong orthogonal array of strength t enjoys better   space-filling properties than a comparable orthogonal array in all   dimensions lower than t while retaining the space-filling properties of   the latter in t dimensions. Latin hypercubes based on strong orthogonal   arrays of strength t are more space-filling than comparable orthogonal   array-based Latin hypercubes in all g dimensions for any 2 < g < t - 1.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Quaternary code (QC) designs form an attractive class of nonregular   factorial fractions. We develop a complementary set theory for   characterizing optimal QC designs that are highly fractionated in the   sense of accommodating a large number of factors. This is in contrast to   existing theoretical results which work only for a relatively small   number of factors. While the use of imaginary numbers to represent the   Gray map associated with QC designs facilitates the derivation,   establishing a link with foldovers of regular fractions helps in   presenting our results in a neat form.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: We develop a new method for constructing ``good'' designs for computer   experiments. The method derives its power from its basic structure that   builds large designs using small designs. We specialize the method for   the construction of orthogonal Latin hypercubes and obtain many results   along the way. In terms of run sizes, the existence problem of   orthogonal Latin hypercubes is completely solved. We also present an   explicit result showing how large orthogonal Latin hypercubes can be   constructed using small orthogonal Latin hypercubes. Another appealing   feature of our method is that it can easily be adapted to construct   other designs; we examine how to make use of the method to construct   nearly orthogonal and cascading Latin hypercubes.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Enumerating nonisomorphic orthogonal arrays is an important, yet very   difficult, problem. Although orthogonal arrays with a specified set of   parameters have been enumerated in a number of cases, general results   are extremely rare. In this paper, we provide a complete solution to   enumerating nonisomorphic two-level orthogonal arrays of strength d with   d + 2 constraints for any d and any run size n = lambda 2(d). Our   results not only give the number of nonisomorphic orthogonal arrays for   given d and n, but also provide a systematic way of explicitly   constructing these arrays. Our approach to the problem is to make use of   the recently developed theory of J-characteristics for fractional   factorial designs. Besides the general theoretical results, the paper   presents some results from applications of the theory to orthogonal   arrays of strength two, three and four.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Minimum aberration is an increasingly popular criterion for comparing   and assessing fractional factorial designs, and few would question its   importance and usefulness nowadays. In the past decade or so, a great   deal of work has been done on minimum aberration and its various   extensions. This paper develops a general theory of minimum aberration   based on a sound statistical principle. Our theory provides a unified   framework for minimum aberration and further extends the existing work   in the area. More importantly, the theory offers a systematic method   that enables experimenters to derive their own aberration criteria. Our   general theory also brings together two seemingly separate research   areas: one on minimum aberration designs and the other on designs with   requirement sets. To facilitate the design construction, we develop a   complementary design theory for quite a general class of aberration   criteria. As an immediate application, we present some construction   results on a weak version of this class of criteria.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Deng and Tang proposed generalized resolution and minimum aberration   criteria for comparing and assessing nonregular fractional factorials,   of which Plackett-Burman designs are special cases. A relaxed variant of   generalized aberration is proposed and studied in this paper. We show   that a best design according to this criterion minimizes the   contamination of nonnegligible interactions on the estimation of main   effects in the order of importance given by the hierarchical assumption.   The new criterion is defined through a set of B values, a generalization   of word length pattern. We derive some theoretical results that relate   the B values of a nonregular fractional factorial and those of its   complementary design. Application of this theory to the construction of   the best designs according to the new aberration criterion is discussed.   The results in this paper generalize those in Tang and Wu, which   characterize a minimum aberration (regular) 2(m-k) design through its   complementary design.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: The construction of run orders of two-level factorial designs with   extreme (minimum and maximum) numbers of level changes is considered.   Minimizing the number of level changes is mainly due to economic   considerations, while the problem of maximizing the number of level   changes arises from some recent results on trend robust designs. The   construction is based on the fact that the 2(k) runs of a saturated   regular fractional factorial design for 2(k) -1 factors can be ordered   in such a way that the numbers of level changes of the factors consist   of each integer between 1 and 2(k) - 1. Among other results, we give a   systematic method of constructing designs with minimum and maximum   numbers of level changes among all designs of resolution at least three   and among those of resolution at least four. It is also shown that among   regular fractional factorial designs of resolution at least four, the   number of level changes can be maximized and minimized by different run   orders of the same fraction.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: A general result is obtained that relates the word-length pattern of a   2(n-k) design to that of its complementary design. By applying this   result and using group isomorphism, we are able to characterize minimum   aberration 2(n-k) designs in terms of properties of their complementary   designs. The approach is quite powerful for small values of 2(n-k) - n -   1. In particular, we obtain minimum aberration 2(n-k) designs with   2(n-k) - n - 1 = 1 to 11 for any n and h.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: In an early paper, He and Tang [Biometrika 100 (2013) 254-260]   introduced and studied a new class of designs, strong orthogonal arrays,   for computer experiments, and characterized such arrays through   generalized orthogonal arrays. The current paper presents a simple   characterization for strong orthogonal arrays of strength three. Besides   being simple, this new characterization through a notion of   semi-embeddability is more direct and penetrating in terms of revealing   the structure of strong orthogonal arrays. Some other results on strong   orthogonal arrays of strength three are also obtained along the way, and   in particular, two SOA(54,5,27,3)'s are constructed.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: In this article, we use orthogonal arrays (OA's) to construct Latin   hypercubes. Besides preserving the univariate stratification properties   of Latin hypercubes, these strength r OA-based Latin hypercubes also   stratify each r-dimensional margin. Therefore, such OA-based Latin   hypercubes provide more suitable designs for computer experiments and   numerical integration than do general Latin hypercubes. We prove that   when used for integration, the sampling scheme with OA-based Latin   hypercubes offers a substantial improvement over Latin hypercube   sampling.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This paper establishes a sufficient and necessary condition for the   existence of an unbiased estimator for a general parameter of a finite   population. It turns out that the condition depends on the collection of   alpha-decomposition sets associated with an alpha-decomposition of the   parameter. It is proved that for any parameter, there always exists an   alpha-decomposition and the associated collection of alpha-decomposition   sets is unique.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: The authors derive upper and lower bounds on the maximum number of clear   two-factor interactions in 2(m-p) fractional factorial designs of   resolution III and IV. A two-factor interaction is said to be clear if   it is not aliased with any main effect or with any other two-factor   interaction. The lower bounds are obtained by exhibiting specific   designs. By comparing the bounds with the values of the maximum number   of clear two-factor interactions in cases where it is known, one   concludes that the construction methods perform quite well.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: A lower bound for the Es-2 value of an arbitrary supersaturated design   is derived. A general method for constructing supersaturated designs is   proposed and shown to produce designs with n runs and m = k(n - 1)   factors that achieve the lower bound for Es-2 and are thus optimal with   respect to the Es-2 criterion. Within the class of designs given by the   construction method, further discrimination can be made by minimizing   the pairwise correlations and using the generalized D and A criteria   proposed by Wu (1993). Efficient designs of 12, 16, 20 and 24 runs are   constructed by following this approach.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Fries and Hunter (1980) proposed the Minimum Aberration Criterion (Mb)   for selecting regular designs. The regular designs with MA are most   commonly used because they are considered as the best designs. However,   as pointed out by Chen, Sun and Wu (1993), there are situations that   other designs may better meet the design need. Therefore, they   catalogued some two-level and three-level fractional factorial regular   designs with small (16,27,32,64) runs. For nonregular designs, such as   the ones taken from Hadamard matrices, the MA criterion is not   applicable. Deng and Tang (1999) introduced Generalized Minimum   Aberration Criterion (GMA) as a natural extension to the MA criterion.   Similar to the case in the regular designs, other designs may better   meet practical need. In this paper, we use the GMA criterion to give a   catalogue of nonregular designs with small (16, 20, 24) runs.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: The maximin distance criterion is used for the selection of an OA-based   Latin hypercube. For the case in which the underlying orthogonal array   is a full factorial design without replication, we construct an OA-based   Latin hypercube that reaches the same distance as its parent array.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: We consider the problem of constructing good two-level nonregular   fractional factorial designs. The criteria of minimum G and G(2)   aberration are used to rank designs. A general design structure is   utilized to provide a solution to this practical, yet challenging,   problem. With the help of this design structure, we develop an efficient   algorithm for obtaining a collection of good designs based on the   aforementioned two criteria. Finally, we present some results for   designs of 32 and 40 runs obtained from applying this algorithmic   approach. (C) 2011 Elsevier BM. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This paper considers constructing minimum aberration blocking schemes   for 2(m) designs. A blocking scheme is said to have estimability of   order e if e is the greatest integer such that no effect involving e or   less factors is aliased with a block effect. We observe that a minimum   aberration blocking scheme with estimability of order e uniquely   corresponds to a minimum aberration design of resolution R = e + 1 and   vice versa. Two implications follow immediately from this result. First,   all existing minimum aberration designs of resolution III or higher can   be used to obtain minimum aberration blocking schemes with estimability   of order 2 or higher. Second, resolution II designs of minimum   aberration are now of both theoretical and practical importance as they   provide solutions to minimum aberration blocking schemes with   estimability of order 1. We study the construction of minimum aberration   designs of resolution II, and obtain a complete solution to the problem.   (c) 2006 Elsevier B.V. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Deng and Tang (Technometrics 44 (2002) 173) constructed a catalog of   designs of 16, 20 and 24 runs using the criterion of minimum   G-aberration, by searching through all orthogonal arrays from Hadamard   matrices. Since it is not true that every orthogonal array can be   embedded into a Hadamard matrix, some good designs may be missing from   their catalog. This paper examines the same problem by considering all   orthogonal arrays. Two advantages result from removing the restriction   to designs from Hadamard matrices. We are able to obtain some results on   designs of run size 28 or higher. More importantly, we have indeed found   minimum G-aberration designs that cannot be embedded into Hadamard   matrices. A catalog of useful designs is presented here and its   usefulness discussed. (C) 2003 Elsevier B.V. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Generalized resolution and minimum aberration were recently proposed and   justified in Deng and Tang (Statist. Sinica 9 (1999) 1071). In this   note, we construct generalized minimum aberration designs of 3, 4, and 5   factors, for any run size n that is a multiple of 4. (C) 2002 Elsevier   Science B.V. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Resampling methods for survey sampling have been broadly classified into   three categories: the jackknife, the bootstrap, and balanced repeated   replication (BRR) in the literature. In this paper, we consider the   balanced bootstrap. Two classes of balanced bootstrap designs are   constructed. The balanced bootstrap method based on the first class of   designs includes ERR as a special case. The second class of designs   generalize those given by Nigam and Rao (1996). (C) 1999 Elsevier   Science B.V. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This paper reviews the formulation of the K-aberration criterion for   baseline two-level designs and the efficient complete search algorithm   developed by Mukerjee and Tang (2012). An efficient incomplete search   algorithm is proposed that can be used to find near optimal baseline   designs in situations where the complete search algorithm is not   feasible. Lower bounds for values of K-2 and K-3 are established. A   catalogue of optimal (or near optimal) 20-run baseline designs is   provided. (C) 2014 Elsevier B.V. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: In some situations, the experimenter would like to study factors at more   than two levels, such as when curvature has the potential to occur   within the experimental region. In this paper we consider selecting   orthogonal arrays with more than two levels, when there is a particular   set of two-factor interaction components of interest to the   experimenter. As designs with more than two levels have additional   complications with level permutations, results are provided that aid in   the search for efficient designs that have robust properties. We also   examine the existence and construction of such designs through saturated   orthogonal arrays. (C) 2013 Elsevier B.V. All rights reserved.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This paper investigates minimal dependent sets used for evaluating   supersaturated designs. Unlike the popular E(s(2)) criterion, the   criteria based on minimal dependent sets directly capture the properties   of designs in terms of estimation and identification of active factors.   This paper provides a theoretical investigation into the number and   structure of minimal dependent sets in a supersaturated design, and   presents some construction results on supersaturated designs with large   minimal dependent sets.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: This paper considers two-level orthogonal arrays that, allow joint   estimation of all main effects and a set of prespecified two-factor   interactions. We obtain some theoretical results that provide a simple   characterization of when such designs exist, and how to construct them   if they do. General as well as concrete applications of the results are   discussed.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Computer experiments with different levels of accuracy have become   prevalent in many engineering and scientific applications. Design   construction for such computer experiments is a new issue because the   existing methods deal almost exclusively with computer experiments with   one level of accuracy. In this paper, we construct some nested   space-filling designs for computer experiments with two levels of   accuracy. Our construction makes use of Galois fields and orthogonal   arrays.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: In a 2(m-p) design of resolution IV, some two-factor interactions   (2fi's) may be important and should be estimated without confounding   with other 2fi's. Four classes of compromise plans that specify certain   2fi's to be important have been discussed in the literature. Compromise   plans are said to be clear if they are of resolution IV and all the   specified 2fi's are clear. A 2fi is clear if it is not aliased with any   main effect or any other 2fi. Clear compromise plans allow joint   estimation of all main effects and these clear 2fi's under the weak   assumption that all three-factor and higher order interactions are   negligible. In this paper, we study the existence and characteristics of   clear compromise plans of classes one to four, and give a catalog of   clear compromise plans of 32 and 64 runs.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Tang and Deng (1999) proposed a generalized minimum aberration criterion   (GMA) as a natural extension of the minimum aberration criterion (MA)   from regular to nonregular designs. While MA was defined for regular   designs only, GMA applies to both regular and nonregular designs. In   this paper, we investigate the relationship between GMA and some   model-dependent efficiency criteria, and show that GMA is supported by   these criteria. An extensive evaluation of designs with 20 runs and 5   factors shows that the GMA criterion. can be used to classify and   rank-order these designs. Empirical studies also demonstrate that the   GMA ranking is consistent with those obtained by using other   model-dependent efficiency criteria.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Resolution has been the most widely used criterion for comparing regular   fractional factorials since it was introduced in 1961 by Box and Hunter.   In this paper, we examine how a generalized resolution criterion can be   defined and used for assessing nonregular fractional factorials, notably   Plackett-Burman designs. Our generalization is intended to capture   projection properties, complementing that of Webb (1964) whose concept   of resolution concerns the estimability of lower order effects under the   assumption that higher order effects are negligible. Our generalized   resolution provides a fruitful criterion for ranking different designs   while Webb's resolution is mainly useful as a classification rule. An   additional advantage of our approach is that the idea leads to a natural   generalization of minimum aberration. Examples are given to illustrate   the usefulness of the new criteria.\n",
      "Embedding: (384,)\n",
      "\n",
      "Sentence: Latin hypercube designs have recently found wide applications both in   design of experiments and in numerical integration. An important   property of this class of designs is that they achieve uniformity in   each univariate margin. In this article we study the use of correlation   criteria to select a Latin hypercube. We introduce the polynomial   canonical correlation of two vectors and argue that a design which has a   small polynomial canonical correlation for each pair of its columns is   preferred. An algorithm for reducing polynomial canonical correlations   of a Latin hypercube is developed. The implementation of the algorithm   is discussed, and its performance investigated. Comparison with Owen's   algorithm is also made.\n",
      "Embedding: (384,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence, embedding in zip(list(author_1.text), embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d5b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(list(author_1.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01f6d33e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(author_1.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "30e5f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.to_csv(\"Hard_pairs_PHC.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "9b7462db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Hard_pairs_chi2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d9663799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author 1</th>\n",
       "      <th>Author 2</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Håvard Rue</td>\n",
       "      <td>Ross Prentice</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.941176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Douglas A. Wolfe</td>\n",
       "      <td>Ian White</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chunsheng Ma</td>\n",
       "      <td>Els Goetghebeur</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.880000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M. Elizabeth Halloran</td>\n",
       "      <td>Susan Lewis</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.962963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gérard Letac</td>\n",
       "      <td>Clarice R. Weinberg</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wolfgang Härdle</td>\n",
       "      <td>Rahul Mukerjee</td>\n",
       "      <td>0.830508</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Friedrich Götze</td>\n",
       "      <td>Bimal Sinha</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.871795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kung-jong Lui</td>\n",
       "      <td>Hu 1 Yang</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.814815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Omer Ozturk</td>\n",
       "      <td>Randy Sitter</td>\n",
       "      <td>0.913043</td>\n",
       "      <td>0.909091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nicholas I. Fisher</td>\n",
       "      <td>Wolfgang Wefelmeyer</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Author 1             Author 2  Accuracy        F1\n",
       "0             Håvard Rue        Ross Prentice  0.921053  0.941176\n",
       "0       Douglas A. Wolfe            Ian White  0.800000  0.750000\n",
       "0           Chunsheng Ma      Els Goetghebeur  0.863636  0.880000\n",
       "0  M. Elizabeth Halloran          Susan Lewis  0.950000  0.962963\n",
       "0           Gérard Letac  Clarice R. Weinberg  1.000000  1.000000\n",
       "0        Wolfgang Härdle       Rahul Mukerjee  0.830508  0.875000\n",
       "0        Friedrich Götze          Bimal Sinha  0.827586  0.871795\n",
       "0          Kung-jong Lui            Hu 1 Yang  0.814815  0.814815\n",
       "0            Omer Ozturk         Randy Sitter  0.913043  0.909091\n",
       "0     Nicholas I. Fisher  Wolfgang Wefelmeyer  0.920000  0.916667"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "31a7246c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING Han-ying Liang AGAINST Ngai Hang Chan\n",
      "Accuracy on test set =  0.5454545454545454\n",
      "f1 score =  0.5454545454545454\n",
      "-----------------------------------------------------------------\n",
      "         Author 1        Author 2  Accuracy        F1\n",
      "0  Han-ying Liang  Ngai Hang Chan  0.545455  0.545455\n",
      "TESTING Louise Ryan AGAINST Kerrie Mengersen\n",
      "Accuracy on test set =  0.5\n",
      "f1 score =  0.43749999999999994\n",
      "-----------------------------------------------------------------\n",
      "         Author 1          Author 2  Accuracy        F1\n",
      "0  Han-ying Liang    Ngai Hang Chan  0.545455  0.545455\n",
      "0     Louise Ryan  Kerrie Mengersen  0.500000  0.437500\n",
      "TESTING Jerzy K. Baksalary AGAINST Zehua Chen\n",
      "Accuracy on test set =  0.5789473684210527\n",
      "f1 score =  0.6\n",
      "-----------------------------------------------------------------\n",
      "             Author 1          Author 2  Accuracy        F1\n",
      "0      Han-ying Liang    Ngai Hang Chan  0.545455  0.545455\n",
      "0         Louise Ryan  Kerrie Mengersen  0.500000  0.437500\n",
      "0  Jerzy K. Baksalary        Zehua Chen  0.578947  0.600000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING Enrique Schisterman AGAINST Paddy Farrington\n",
      "Accuracy on test set =  0.5238095238095238\n",
      "f1 score =  0.5454545454545454\n",
      "-----------------------------------------------------------------\n",
      "              Author 1          Author 2  Accuracy        F1\n",
      "0       Han-ying Liang    Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan  Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary        Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman  Paddy Farrington  0.523810  0.545455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING Brian Caffo AGAINST Lyle Broemeling\n",
      "Accuracy on test set =  0.55\n",
      "f1 score =  0.5263157894736842\n",
      "-----------------------------------------------------------------\n",
      "              Author 1          Author 2  Accuracy        F1\n",
      "0       Han-ying Liang    Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan  Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary        Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman  Paddy Farrington  0.523810  0.545455\n",
      "0          Brian Caffo   Lyle Broemeling  0.550000  0.526316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING Myles Hollander AGAINST Victor De Gruttola\n",
      "Accuracy on test set =  0.5\n",
      "f1 score =  0.56\n",
      "-----------------------------------------------------------------\n",
      "              Author 1            Author 2  Accuracy        F1\n",
      "0       Han-ying Liang      Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan    Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary          Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman    Paddy Farrington  0.523810  0.545455\n",
      "0          Brian Caffo     Lyle Broemeling  0.550000  0.526316\n",
      "0      Myles Hollander  Victor De Gruttola  0.500000  0.560000\n",
      "TESTING Paul Gustafson AGAINST Noel Cressie\n",
      "Accuracy on test set =  0.5\n",
      "f1 score =  0.4864864864864865\n",
      "-----------------------------------------------------------------\n",
      "              Author 1            Author 2  Accuracy        F1\n",
      "0       Han-ying Liang      Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan    Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary          Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman    Paddy Farrington  0.523810  0.545455\n",
      "0          Brian Caffo     Lyle Broemeling  0.550000  0.526316\n",
      "0      Myles Hollander  Victor De Gruttola  0.500000  0.560000\n",
      "0       Paul Gustafson        Noel Cressie  0.500000  0.486486\n",
      "TESTING Atanu Biswas AGAINST Noël Veraverbeke\n",
      "Accuracy on test set =  0.5862068965517241\n",
      "f1 score =  0.5714285714285714\n",
      "-----------------------------------------------------------------\n",
      "              Author 1            Author 2  Accuracy        F1\n",
      "0       Han-ying Liang      Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan    Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary          Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman    Paddy Farrington  0.523810  0.545455\n",
      "0          Brian Caffo     Lyle Broemeling  0.550000  0.526316\n",
      "0      Myles Hollander  Victor De Gruttola  0.500000  0.560000\n",
      "0       Paul Gustafson        Noel Cressie  0.500000  0.486486\n",
      "0         Atanu Biswas    Noël Veraverbeke  0.586207  0.571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING Robert Kohn AGAINST Paul Janssen\n",
      "Accuracy on test set =  0.5714285714285714\n",
      "f1 score =  0.5714285714285714\n",
      "-----------------------------------------------------------------\n",
      "              Author 1            Author 2  Accuracy        F1\n",
      "0       Han-ying Liang      Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan    Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary          Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman    Paddy Farrington  0.523810  0.545455\n",
      "0          Brian Caffo     Lyle Broemeling  0.550000  0.526316\n",
      "0      Myles Hollander  Victor De Gruttola  0.500000  0.560000\n",
      "0       Paul Gustafson        Noel Cressie  0.500000  0.486486\n",
      "0         Atanu Biswas    Noël Veraverbeke  0.586207  0.571429\n",
      "0          Robert Kohn        Paul Janssen  0.571429  0.571429\n",
      "TESTING David Schoenfeld AGAINST Stuart J. Pocock\n",
      "Accuracy on test set =  0.6\n",
      "f1 score =  0.5555555555555556\n",
      "-----------------------------------------------------------------\n",
      "              Author 1            Author 2  Accuracy        F1\n",
      "0       Han-ying Liang      Ngai Hang Chan  0.545455  0.545455\n",
      "0          Louise Ryan    Kerrie Mengersen  0.500000  0.437500\n",
      "0   Jerzy K. Baksalary          Zehua Chen  0.578947  0.600000\n",
      "0  Enrique Schisterman    Paddy Farrington  0.523810  0.545455\n",
      "0          Brian Caffo     Lyle Broemeling  0.550000  0.526316\n",
      "0      Myles Hollander  Victor De Gruttola  0.500000  0.560000\n",
      "0       Paul Gustafson        Noel Cressie  0.500000  0.486486\n",
      "0         Atanu Biswas    Noël Veraverbeke  0.586207  0.571429\n",
      "0          Robert Kohn        Paul Janssen  0.571429  0.571429\n",
      "0     David Schoenfeld    Stuart J. Pocock  0.600000  0.555556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n",
      "/var/folders/rb/kxt2pvjx7bb3mg6mbyj8fhzc0000gn/T/ipykernel_69242/2666481320.py:54: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(df1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "#     author1 = \"Aiyi Liu\"\n",
    "#     author2 = \"David Cox\"\n",
    "    df = pd.DataFrame()\n",
    "    hard_pairs = []\n",
    "    while len(df) < 10:\n",
    "        author1 = random.choice(author_l)\n",
    "        author2 = random.choice(author_l)\n",
    "        if author1!=author2 and count_shared_papers(author1,author2,authors,data)==0:\n",
    "\n",
    "            author_1 = pd.read_csv(f'../Data/{author1}.csv').filter(['author', 'title', 'text'])\n",
    "            author_2 = pd.read_csv(f'../Data/{author2}.csv').filter(['author', 'title', 'text'])\n",
    "            n, m = author_1.shape[0], author_2.shape[0]\n",
    "            if min(n/m, m/n) >= 1/2:\n",
    "                data_ = pd.concat([clean_text(author_1),\n",
    "                              clean_text(author_2)], ignore_index=True)\n",
    "\n",
    "                data_train = data_.sample(frac=0.7)\n",
    "                data_test = data_.drop(data_train.index)\n",
    "                vocab = get_vocab(''.join([doc for doc in list(data_train[\"text\"])]))\n",
    "\n",
    "                text1 = data_train[data_train[\"author\"]==author1]\n",
    "                text2 = data_train[data_train[\"author\"]==author2]\n",
    "\n",
    "                text1\n",
    "                #corpus1 = doc_to_dtm([\"\".join(list(text1.text))],vocab=vocab)\n",
    "                corpus1 = doc_to_dtm(list(text1.text),vocab=vocab)\n",
    "                corpus2 = doc_to_dtm(list(text2.text),vocab=vocab)\n",
    "\n",
    "                lam_1 = estimate_poisson(corpus1)\n",
    "                lam_2 = estimate_poisson(corpus2)\n",
    "                y_pred = []\n",
    "                for doc in list(data_test[\"text\"]):\n",
    "                    dtm = doc_to_dtm([doc],vocab=vocab)\n",
    "                    if np.sum((dtm - lam_1)**2) < np.sum((dtm - lam_2)**2):\n",
    "                        y_pred.append(author1)\n",
    "                    else:\n",
    "                        y_pred.append(author2)\n",
    "\n",
    "\n",
    "                \"\"\"Accuracy and F1 score on test set\"\"\"\n",
    "                y_true = list(data_test[\"author\"])\n",
    "                y_pred = [0 if item==author1 else 1 for item in y_pred]\n",
    "                y_true = [0 if item==author1 else 1 for item in y_true]\n",
    "                acc = np.mean(np.array(y_pred)==np.array(y_true))\n",
    "                f1 = f1_score(y_pred, y_true)\n",
    "                if acc <= 0.6 and f1 <= 0.6:\n",
    "                    hard_pairs.append((author1,author2))\n",
    "                    print(f\"TESTING {author1} AGAINST {author2}\")\n",
    "                    print(\"Accuracy on test set = \",np.mean(np.array(y_pred)==np.array(y_true)))\n",
    "                    print(\"f1 score = \",f1_score(y_pred, y_true))\n",
    "                    print(\"-----------------------------------------------------------------\")\n",
    "                    df1 = pd.DataFrame({\"Author 1\":author1,\"Author 2\":author2,\"Accuracy\":acc,\"F1\":f1},index=[0])\n",
    "                    df = df.append(df1)\n",
    "                    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b47d67c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hard_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhard_pairs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hard_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "hard_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff6ae653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'of', 'a', 'to', 'and', 'is', 'in', 'for', 'we', 'are',\n",
       "       'with', 'data', 'model', 'that', 'as', 'models', 'this', 'on',\n",
       "       'be', 'an', 'spatial', 'by', 'process', 'from', 'using',\n",
       "       'approach', 'which', 'at', 'bayesian', 'can', 'such',\n",
       "       'distribution', 'two', 'or', 'random', 'mixture', 'each', 'our',\n",
       "       'effects', 'it', 'modeling', 'analysis', 'these', 'given',\n",
       "       'regression', 'used', 'algorithm', 'set', 'hierarchical', 'one',\n",
       "       'posterior', 'distributions', 'inference', 'sample', 'fitting',\n",
       "       'not', 'problem', 'species', 'have', 'where', 'level', 'also',\n",
       "       'when', 'information', 'methods', 'more', 'proposed', 'error',\n",
       "       'results', 'based', 'observed', 'performance', 'been', 'function',\n",
       "       'interest', 'number', 'over', 'well', 'class', 'functions',\n",
       "       'population', 'provide', 'sampling', 'through', 'both', 'case',\n",
       "       'conditional', 'estimation', 'has', 'illustrate', 'paper',\n",
       "       'probability', 'test', 'variables', 'work', 'between',\n",
       "       'particular', 'region', 'risk', 'simulation', 'use', 'c',\n",
       "       'develop', 'dirichlet', 'estimates', 'factors', 'linear',\n",
       "       'multivariate', 'observations', 'other', 'prior', 'specification',\n",
       "       'then', 'time', 'within', 'associated', 'cancer', 'context',\n",
       "       'first', 'large', 'method', 'n', 'rate', 'rates', 'show', 'some',\n",
       "       'survival', 'approaches', 'available', 'but', 'here', 'how',\n",
       "       'however', 'individual', 'only', 'patterns', 'present', 'sets',\n",
       "       'under', 'across', 'all', 'carlo', 'clustering', 'considered',\n",
       "       'different', 'form', 'framework', 'its', 'levels', 'monte',\n",
       "       'normal', 'often', 'propose', 'simulated', 'there', 'unknown',\n",
       "       'age', 'chain', 'dataset', 'family', 'fitted', 'fully', 'joint',\n",
       "       'likelihood', 'many', 'may', 'provides', 'range', 'size', 'cell',\n",
       "       'consider', 'gaussian', 'into', 'markov', 'nonparametric', 'point',\n",
       "       'predictive', 'radiologist', 'study', 'than', 'about', 'abundance',\n",
       "       'application', 'developed', 'environmental', 'mean', 'mixing',\n",
       "       'populations', 'probabilities', 'real', 'semiparametric',\n",
       "       'several', 'space', 'specified', 'components', 'criterion',\n",
       "       'illustrated', 'locations', 'parameters', 'parametric',\n",
       "       'prediction', 'selection', 'technique', 'thus', 'will',\n",
       "       'applications', 'behavior', 'cells', 'collection', 'copyright',\n",
       "       'generalized', 'involving', 'john', 'ltd', 'modelling', 'most',\n",
       "       'new', 'order', 'resulting', 'samples', 'screening', 'so', 'sons',\n",
       "       'various', 'whether', 'wiley', 'arises', 'attractive', 'basis',\n",
       "       'bias', 'coefficients', 'comparison', 'curves', 'em', 'estimated',\n",
       "       'examples', 'general', 'important', 'location', 'loss', 'marginal',\n",
       "       'obtained', 'presence', 'presented', 'priors', 'regard', 'second',\n",
       "       'setting', 'specifications', 'stage', 'statistic', 'stochastic',\n",
       "       'term', 'they', 'treatment', 'value', 'was', 'applied',\n",
       "       'assessment', 'because', 'counts', 'covariates', 'discuss', 'fit',\n",
       "       'frequencies', 'full', 'groups', 'if', 'intervals', 'introduce',\n",
       "       'latent', 'limited', 'matrix', 'mixtures', 'monitoring', 'obtain',\n",
       "       'ozone', 'patient', 'problems', 'procedure', 'projection',\n",
       "       'response', 'rule', 'scale', 'shown', 'site', 'small', 'spatially',\n",
       "       'typically', 'upon', 'variable', 'wave', 'while', 'allocation',\n",
       "       'allows', 'attributes', 'choice', 'computational', 'curve',\n",
       "       'customary', 'densities', 'dependence', 'discussed', 'elsevier',\n",
       "       'enable', 'errors', 'estimate', 'failure', 'flexible', 'gelfand',\n",
       "       'genes', 'grid', 'iem', 'incorporating', 'maximum', 'mixed',\n",
       "       'multilevel', 'offer', 'patients', 'poisson', 'potential',\n",
       "       'properties', 'received', 'standard', 'stationary', 'structure',\n",
       "       'surface', 'their', 'three', 'us', 'via', 'viewed', 'account',\n",
       "       'allow', 'appropriate', 'article', 'assess', 'autoregressive',\n",
       "       'bayes', 'block', 'bv', 'capture', 'cases', 'cluster', 'commonly',\n",
       "       'component', 'continuous', 'covariance', 'covariate',\n",
       "       'demonstrated', 'desired', 'differences', 'diversity',\n",
       "       'eligibility', 'enables', 'example', 'explain', 'exponential',\n",
       "       'finally', 'functionals', 'further', 'gestational', 'gibbs',\n",
       "       'hazard', 'highdimensional', 'implemented', 'includes',\n",
       "       'increased', 'increasing', 'initial', 'local', 'logistic', 'maps',\n",
       "       'mcmc', 'measurement', 'median', 'methodology', 'outcome',\n",
       "       'outcomes', 'parameter', 'pattern', 'plant', 'rather', 'recent',\n",
       "       'regarding', 'still', 'studies', 'summaries', 'surfaces',\n",
       "       'temporal', 'theta', 'transformation', 'usually', 'variance',\n",
       "       'very', 'what', 'wide', 'widely', 'additional', 'allele'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvals = 1 - norm.cdf(z_n)\n",
    "HC = HC(list(pvals))\n",
    "np.array(vocab)[pvals <= HC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "855173c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(author_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
